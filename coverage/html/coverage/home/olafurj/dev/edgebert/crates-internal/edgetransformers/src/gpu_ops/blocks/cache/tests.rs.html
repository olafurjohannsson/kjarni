<!doctype html><html><head><meta name='viewport' content='width=device-width,initial-scale=1'><meta charset='UTF-8'><link rel='stylesheet' type='text/css' href='../../../../../../../../../../../style.css'><script src='../../../../../../../../../../../control.js'></script></head><body><h2>Coverage Report</h2><h4>Created: 2025-12-04 22:30</h4><span class='control'><a href='javascript:next_line()'>next uncovered line (L)</a>, <a href='javascript:next_region()'>next uncovered region (R)</a>, <a href='javascript:next_branch()'>next uncovered branch (B)</a></span><div class='centered'><table><div class='source-name-title'><pre>/home/olafurj/dev/edgebert/crates-internal/edgetransformers/src/gpu_ops/blocks/cache/tests.rs</pre></div><tr><td><pre>Line</pre></td><td><pre>Count</pre></td><td><pre>Source</pre></td></tr><tr><td class='line-number'><a name='L1' href='#L1'><pre>1</pre></a></td><td class='skipped-line'></td><td class='code'><pre>use super::GpuUpdateCache;</pre></td></tr><tr><td class='line-number'><a name='L2' href='#L2'><pre>2</pre></a></td><td class='skipped-line'></td><td class='code'><pre>use crate::gpu_context::WgpuContext;</pre></td></tr><tr><td class='line-number'><a name='L3' href='#L3'><pre>3</pre></a></td><td class='skipped-line'></td><td class='code'><pre>use crate::gpu_ops::GpuTensor;</pre></td></tr><tr><td class='line-number'><a name='L4' href='#L4'><pre>4</pre></a></td><td class='skipped-line'></td><td class='code'><pre>use anyhow::Result;</pre></td></tr><tr><td class='line-number'><a name='L5' href='#L5'><pre>5</pre></a></td><td class='skipped-line'></td><td class='code'><pre>use ndarray::{Array, Array3, Array4, Ix4, s};</pre></td></tr><tr><td class='line-number'><a name='L6' href='#L6'><pre>6</pre></a></td><td class='skipped-line'></td><td class='code'><pre>use std::sync::Arc;</pre></td></tr><tr><td class='line-number'><a name='L7' href='#L7'><pre>7</pre></a></td><td class='skipped-line'></td><td class='code'><pre></pre></td></tr><tr><td class='line-number'><a name='L8' href='#L8'><pre>8</pre></a></td><td class='skipped-line'></td><td class='code'><pre>// Helper to get a test context.</pre></td></tr><tr><td class='line-number'><a name='L9' href='#L9'><pre>9</pre></a></td><td class='covered-line'><pre>1</pre></td><td class='code'><pre>async fn get_test_context() -&gt; Arc&lt;WgpuContext&gt; {</pre></td></tr><tr><td class='line-number'><a name='L10' href='#L10'><pre>10</pre></a></td><td class='covered-line'><pre>1</pre></td><td class='code'><pre>    WgpuContext::new().await.unwrap()</pre></td></tr><tr><td class='line-number'><a name='L11' href='#L11'><pre>11</pre></a></td><td class='covered-line'><pre>1</pre></td><td class='code'><pre>}</pre></td></tr><tr><td class='line-number'><a name='L12' href='#L12'><pre>12</pre></a></td><td class='skipped-line'></td><td class='code'><pre></pre></td></tr><tr><td class='line-number'><a name='L13' href='#L13'><pre>13</pre></a></td><td class='skipped-line'></td><td class='code'><pre>// Helper to read a 4D GPU tensor back to the CPU for comparison.</pre></td></tr><tr><td class='line-number'><a name='L14' href='#L14'><pre>14</pre></a></td><td class='covered-line'><pre>1</pre></td><td class='code'><pre>async fn read_gpu_tensor_4d(tensor: &amp;GpuTensor) -&gt; Result&lt;Array4&lt;f32&gt;&gt; {</pre></td></tr><tr><td class='line-number'><a name='L15' href='#L15'><pre>15</pre></a></td><td class='covered-line'><pre>1</pre></td><td class='code'><pre>    let shape = tensor.shape();</pre></td></tr><tr><td class='line-number'><a name='L16' href='#L16'><pre>16</pre></a></td><td class='covered-line'><pre>1</pre></td><td class='code'><pre>    let raw_data = tensor.read_raw_data().await<div class='tooltip'><span class='region red'>?</span><span class='tooltip-content'>0</span></div>;</pre></td></tr><tr><td class='line-number'><a name='L17' href='#L17'><pre>17</pre></a></td><td class='covered-line'><pre>1</pre></td><td class='code'><pre>    let data_slice: &amp;[f32] = bytemuck::cast_slice(&amp;raw_data);</pre></td></tr><tr><td class='line-number'><a name='L18' href='#L18'><pre>18</pre></a></td><td class='covered-line'><pre>1</pre></td><td class='code'><pre>    Ok(Array::from_shape_vec(</pre></td></tr><tr><td class='line-number'><a name='L19' href='#L19'><pre>19</pre></a></td><td class='covered-line'><pre>1</pre></td><td class='code'><pre>        (shape[0], shape[1], shape[2], shape[3]),</pre></td></tr><tr><td class='line-number'><a name='L20' href='#L20'><pre>20</pre></a></td><td class='covered-line'><pre>1</pre></td><td class='code'><pre>        data_slice.to_vec(),</pre></td></tr><tr><td class='line-number'><a name='L21' href='#L21'><pre>21</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>    )<span class='region red'>?</span>)</pre></td></tr><tr><td class='line-number'><a name='L22' href='#L22'><pre>22</pre></a></td><td class='covered-line'><pre>1</pre></td><td class='code'><pre>}</pre></td></tr><tr><td class='line-number'><a name='L23' href='#L23'><pre>23</pre></a></td><td class='skipped-line'></td><td class='code'><pre></pre></td></tr><tr><td class='line-number'><a name='L24' href='#L24'><pre>24</pre></a></td><td class='skipped-line'></td><td class='code'><pre>#[tokio::test]</pre></td></tr><tr><td class='line-number'><a name='L25' href='#L25'><pre>25</pre></a></td><td class='covered-line'><pre>1</pre></td><td class='code'><pre>async fn test_update_cache_kernel() -&gt; Result&lt;()&gt; {</pre></td></tr><tr><td class='line-number'><a name='L26' href='#L26'><pre>26</pre></a></td><td class='covered-line'><pre>1</pre></td><td class='code'><pre>    println!(&quot;\n--- Testing GpuUpdateCache Kernel Logic ---&quot;);</pre></td></tr><tr><td class='line-number'><a name='L27' href='#L27'><pre>27</pre></a></td><td class='covered-line'><pre>1</pre></td><td class='code'><pre>    let context = get_test_context().await;</pre></td></tr><tr><td class='line-number'><a name='L28' href='#L28'><pre>28</pre></a></td><td class='covered-line'><pre>1</pre></td><td class='code'><pre>    let kernel = GpuUpdateCache::new(&amp;context);</pre></td></tr><tr><td class='line-number'><a name='L29' href='#L29'><pre>29</pre></a></td><td class='skipped-line'></td><td class='code'><pre></pre></td></tr><tr><td class='line-number'><a name='L30' href='#L30'><pre>30</pre></a></td><td class='skipped-line'></td><td class='code'><pre>    // 1. ARRANGE</pre></td></tr><tr><td class='line-number'><a name='L31' href='#L31'><pre>31</pre></a></td><td class='covered-line'><pre>1</pre></td><td class='code'><pre>    let (b, s_new, h, d) = (1, 1, 4, 4); // Batch, New Tokens, NumHeads, HeadDim</pre></td></tr><tr><td class='line-number'><a name='L32' href='#L32'><pre>32</pre></a></td><td class='covered-line'><pre>1</pre></td><td class='code'><pre>    let hidden_size = h * d; // 16</pre></td></tr><tr><td class='line-number'><a name='L33' href='#L33'><pre>33</pre></a></td><td class='covered-line'><pre>1</pre></td><td class='code'><pre>    let capacity = 8;</pre></td></tr><tr><td class='line-number'><a name='L34' href='#L34'><pre>34</pre></a></td><td class='covered-line'><pre>1</pre></td><td class='code'><pre>    let position_offset = 3; // We are writing the 4th token (index 3)</pre></td></tr><tr><td class='line-number'><a name='L35' href='#L35'><pre>35</pre></a></td><td class='skipped-line'></td><td class='code'><pre></pre></td></tr><tr><td class='line-number'><a name='L36' href='#L36'><pre>36</pre></a></td><td class='skipped-line'></td><td class='code'><pre>    // Create the 3D &quot;new key&quot; tensor with predictable data</pre></td></tr><tr><td class='line-number'><a name='L37' href='#L37'><pre>37</pre></a></td><td class='covered-line'><pre>16</pre></td><td class='code'><pre>    let <div class='tooltip'>new_k_cpu<span class='tooltip-content'>1</span></div> = <div class='tooltip'>Array3::from_shape_fn<span class='tooltip-content'>1</span></div>(<div class='tooltip'>(<span class='tooltip-content'>1</span></div><div class='tooltip'>b<span class='tooltip-content'>1</span></div>, s_new, hidden_size), |(_, _, k)| (k + 1) as f32);</pre></td></tr><tr><td class='line-number'><a name='L38' href='#L38'><pre>38</pre></a></td><td class='covered-line'><pre>1</pre></td><td class='code'><pre>    let new_k_gpu = GpuTensor::from_ndarray(&amp;context, &amp;new_k_cpu)<div class='tooltip'><span class='region red'>?</span><span class='tooltip-content'>0</span></div>;</pre></td></tr><tr><td class='line-number'><a name='L39' href='#L39'><pre>39</pre></a></td><td class='skipped-line'></td><td class='code'><pre>    // A dummy tensor for the &apos;new_v&apos; argument, as we only need to test &apos;k&apos;</pre></td></tr><tr><td class='line-number'><a name='L40' href='#L40'><pre>40</pre></a></td><td class='covered-line'><pre>1</pre></td><td class='code'><pre>    let dummy_v_gpu = GpuTensor::from_ndarray(&amp;context, &amp;new_k_cpu)<div class='tooltip'><span class='region red'>?</span><span class='tooltip-content'>0</span></div>;</pre></td></tr><tr><td class='line-number'><a name='L41' href='#L41'><pre>41</pre></a></td><td class='skipped-line'></td><td class='code'><pre></pre></td></tr><tr><td class='line-number'><a name='L42' href='#L42'><pre>42</pre></a></td><td class='skipped-line'></td><td class='code'><pre>    // Create the 4D cache tensor, initialized to zeros</pre></td></tr><tr><td class='line-number'><a name='L43' href='#L43'><pre>43</pre></a></td><td class='covered-line'><pre>1</pre></td><td class='code'><pre>    let cache_shape = vec![b, h, capacity, d];</pre></td></tr><tr><td class='line-number'><a name='L44' href='#L44'><pre>44</pre></a></td><td class='covered-line'><pre>1</pre></td><td class='code'><pre>    let cache_k_gpu = GpuTensor::uninitialized(</pre></td></tr><tr><td class='line-number'><a name='L45' href='#L45'><pre>45</pre></a></td><td class='covered-line'><pre>1</pre></td><td class='code'><pre>        &amp;context,</pre></td></tr><tr><td class='line-number'><a name='L46' href='#L46'><pre>46</pre></a></td><td class='covered-line'><pre>1</pre></td><td class='code'><pre>        cache_shape,</pre></td></tr><tr><td class='line-number'><a name='L47' href='#L47'><pre>47</pre></a></td><td class='covered-line'><pre>1</pre></td><td class='code'><pre>        crate::gpu_ops::DType::F32,</pre></td></tr><tr><td class='line-number'><a name='L48' href='#L48'><pre>48</pre></a></td><td class='covered-line'><pre>1</pre></td><td class='code'><pre>        &quot;Test K Cache&quot;,</pre></td></tr><tr><td class='line-number'><a name='L49' href='#L49'><pre>49</pre></a></td><td class='skipped-line'></td><td class='code'><pre>    );</pre></td></tr><tr><td class='line-number'><a name='L50' href='#L50'><pre>50</pre></a></td><td class='covered-line'><pre>1</pre></td><td class='code'><pre>    let dummy_cache_v_gpu = cache_k_gpu.clone(); // Dummy for the kernel call</pre></td></tr><tr><td class='line-number'><a name='L51' href='#L51'><pre>51</pre></a></td><td class='skipped-line'></td><td class='code'><pre></pre></td></tr><tr><td class='line-number'><a name='L52' href='#L52'><pre>52</pre></a></td><td class='skipped-line'></td><td class='code'><pre>    // 2. ACT: Run the kernel to write `new_k_gpu` into `cache_k_gpu` at the offset</pre></td></tr><tr><td class='line-number'><a name='L53' href='#L53'><pre>53</pre></a></td><td class='covered-line'><pre>1</pre></td><td class='code'><pre>    let mut encoder = context.device.create_command_encoder(&amp;Default::default());</pre></td></tr><tr><td class='line-number'><a name='L54' href='#L54'><pre>54</pre></a></td><td class='covered-line'><pre>1</pre></td><td class='code'><pre>    kernel.encode(</pre></td></tr><tr><td class='line-number'><a name='L55' href='#L55'><pre>55</pre></a></td><td class='covered-line'><pre>1</pre></td><td class='code'><pre>        &amp;mut encoder,</pre></td></tr><tr><td class='line-number'><a name='L56' href='#L56'><pre>56</pre></a></td><td class='covered-line'><pre>1</pre></td><td class='code'><pre>        &amp;new_k_gpu,</pre></td></tr><tr><td class='line-number'><a name='L57' href='#L57'><pre>57</pre></a></td><td class='covered-line'><pre>1</pre></td><td class='code'><pre>        &amp;dummy_v_gpu,</pre></td></tr><tr><td class='line-number'><a name='L58' href='#L58'><pre>58</pre></a></td><td class='covered-line'><pre>1</pre></td><td class='code'><pre>        &amp;cache_k_gpu,</pre></td></tr><tr><td class='line-number'><a name='L59' href='#L59'><pre>59</pre></a></td><td class='covered-line'><pre>1</pre></td><td class='code'><pre>        &amp;dummy_cache_v_gpu,</pre></td></tr><tr><td class='line-number'><a name='L60' href='#L60'><pre>60</pre></a></td><td class='covered-line'><pre>1</pre></td><td class='code'><pre>        position_offset,</pre></td></tr><tr><td class='line-number'><a name='L61' href='#L61'><pre>61</pre></a></td><td class='skipped-line'></td><td class='code'><pre>    );</pre></td></tr><tr><td class='line-number'><a name='L62' href='#L62'><pre>62</pre></a></td><td class='covered-line'><pre>1</pre></td><td class='code'><pre>    context.queue.submit(Some(encoder.finish()));</pre></td></tr><tr><td class='line-number'><a name='L63' href='#L63'><pre>63</pre></a></td><td class='skipped-line'></td><td class='code'><pre></pre></td></tr><tr><td class='line-number'><a name='L64' href='#L64'><pre>64</pre></a></td><td class='skipped-line'></td><td class='code'><pre>    // 3. ASSERT</pre></td></tr><tr><td class='line-number'><a name='L65' href='#L65'><pre>65</pre></a></td><td class='skipped-line'></td><td class='code'><pre>    // Manually construct the expected result on the CPU.</pre></td></tr><tr><td class='line-number'><a name='L66' href='#L66'><pre>66</pre></a></td><td class='covered-line'><pre>1</pre></td><td class='code'><pre>    let mut expected_cpu = Array4::&lt;f32&gt;::zeros((b, h, capacity, d));</pre></td></tr><tr><td class='line-number'><a name='L67' href='#L67'><pre>67</pre></a></td><td class='skipped-line'></td><td class='code'><pre>    // The kernel should have split the heads of `new_k_cpu` and written them</pre></td></tr><tr><td class='line-number'><a name='L68' href='#L68'><pre>68</pre></a></td><td class='skipped-line'></td><td class='code'><pre>    // into the slice at `position_offset`.</pre></td></tr><tr><td class='line-number'><a name='L69' href='#L69'><pre>69</pre></a></td><td class='covered-line'><pre>4</pre></td><td class='code'><pre>    for head_idx in 0..<div class='tooltip'>h<span class='tooltip-content'>1</span></div> {</pre></td></tr><tr><td class='line-number'><a name='L70' href='#L70'><pre>70</pre></a></td><td class='covered-line'><pre>16</pre></td><td class='code'><pre>        for head_dim_idx in 0..<div class='tooltip'>d<span class='tooltip-content'>4</span></div> {</pre></td></tr><tr><td class='line-number'><a name='L71' href='#L71'><pre>71</pre></a></td><td class='covered-line'><pre>16</pre></td><td class='code'><pre>            let original_idx = head_idx * d + head_dim_idx;</pre></td></tr><tr><td class='line-number'><a name='L72' href='#L72'><pre>72</pre></a></td><td class='covered-line'><pre>16</pre></td><td class='code'><pre>            expected_cpu[[0, head_idx, position_offset, head_dim_idx]] = (original_idx + 1) as f32;</pre></td></tr><tr><td class='line-number'><a name='L73' href='#L73'><pre>73</pre></a></td><td class='covered-line'><pre>16</pre></td><td class='code'><pre>        }</pre></td></tr><tr><td class='line-number'><a name='L74' href='#L74'><pre>74</pre></a></td><td class='skipped-line'></td><td class='code'><pre>    }</pre></td></tr><tr><td class='line-number'><a name='L75' href='#L75'><pre>75</pre></a></td><td class='skipped-line'></td><td class='code'><pre></pre></td></tr><tr><td class='line-number'><a name='L76' href='#L76'><pre>76</pre></a></td><td class='skipped-line'></td><td class='code'><pre>    // Read the actual result back from the GPU.</pre></td></tr><tr><td class='line-number'><a name='L77' href='#L77'><pre>77</pre></a></td><td class='covered-line'><pre>1</pre></td><td class='code'><pre>    let result_gpu = read_gpu_tensor_4d(&amp;cache_k_gpu).await<div class='tooltip'><span class='region red'>?</span><span class='tooltip-content'>0</span></div>;</pre></td></tr><tr><td class='line-number'><a name='L78' href='#L78'><pre>78</pre></a></td><td class='skipped-line'></td><td class='code'><pre></pre></td></tr><tr><td class='line-number'><a name='L79' href='#L79'><pre>79</pre></a></td><td class='skipped-line'></td><td class='code'><pre>    // Compare. Since it&apos;s a direct copy, they should be bit-for-bit identical.</pre></td></tr><tr><td class='line-number'><a name='L80' href='#L80'><pre>80</pre></a></td><td class='covered-line'><pre>1</pre></td><td class='code'><pre>    assert_eq!(</pre></td></tr><tr><td class='line-number'><a name='L81' href='#L81'><pre>81</pre></a></td><td class='covered-line'><pre>1</pre></td><td class='code'><pre>        expected_cpu.as_slice(),</pre></td></tr><tr><td class='line-number'><a name='L82' href='#L82'><pre>82</pre></a></td><td class='covered-line'><pre>1</pre></td><td class='code'><pre>        result_gpu.as_slice(),</pre></td></tr><tr><td class='line-number'><a name='L83' href='#L83'><pre>83</pre></a></td><td class='uncovered-line'><pre>0</pre></td><td class='code'><pre>        <span class='region red'>&quot;The GpuUpdateCache kernel did not write the data to the correct location.&quot;</span></pre></td></tr><tr><td class='line-number'><a name='L84' href='#L84'><pre>84</pre></a></td><td class='skipped-line'></td><td class='code'><pre>    );</pre></td></tr><tr><td class='line-number'><a name='L85' href='#L85'><pre>85</pre></a></td><td class='skipped-line'></td><td class='code'><pre></pre></td></tr><tr><td class='line-number'><a name='L86' href='#L86'><pre>86</pre></a></td><td class='covered-line'><pre>1</pre></td><td class='code'><pre>    println!(&quot;âœ… GpuUpdateCache kernel test passed!&quot;);</pre></td></tr><tr><td class='line-number'><a name='L87' href='#L87'><pre>87</pre></a></td><td class='covered-line'><pre>2</pre></td><td class='code'><pre>    Ok(())</pre></td></tr><tr><td class='line-number'><a name='L88' href='#L88'><pre>88</pre></a></td><td class='covered-line'><pre>1</pre></td><td class='code'><pre>}</pre></td></tr></table></div></body></html>