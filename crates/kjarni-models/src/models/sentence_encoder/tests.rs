#[cfg(test)]
mod sentence_encoder_loading_tests {
    use crate::SentenceEncoder;

    
    use anyhow::Result;
    use kjarni_transformers::models::ModelType;
    use kjarni_transformers::traits::Device;
    use kjarni_transformers::{LanguageModel, WgpuContext};
    use safetensors::tensor::{Dtype, TensorView};
    use std::collections::HashMap;
    
    
    use tempfile::TempDir;

    fn create_dummy_bert_files() -> Result<TempDir> {
        let dir = tempfile::tempdir()?;
        let path = dir.path();

        // 1. Create dummy weights
        let mut tensors = HashMap::new();

        // Define data buffers first
        // Embedding buffer (10 * 4 = 40 floats)
        let shape_emb = vec![10, 4];
        let data_emb = vec![0.0f32; 40];
        let bytes_emb: Vec<u8> = data_emb.iter().flat_map(|f| f.to_le_bytes()).collect();

        // Position embedding buffer (512 * 4 = 2048 floats)
        let data_pos = vec![0.0f32; 512 * 4];
        let bytes_pos: Vec<u8> = data_pos.iter().flat_map(|f| f.to_le_bytes()).collect();

        // Layer weights (4 * 4 = 16 floats)
        let shape_layer = vec![4, 4];
        let data_layer = vec![0.0f32; 16];
        let bytes_layer: Vec<u8> = data_layer.iter().flat_map(|f| f.to_le_bytes()).collect();

        // Biases (4 floats)
        let shape_bias = vec![4];
        let data_bias = vec![0.0f32; 4];
        let bytes_bias: Vec<u8> = data_bias.iter().flat_map(|f| f.to_le_bytes()).collect();

        tensors.insert(
            "embeddings.word_embeddings.weight".to_string(),
            TensorView::new(Dtype::F32, shape_emb.clone(), &bytes_emb)?,
        );

        tensors.insert(
            "embeddings.position_embeddings.weight".to_string(),
            TensorView::new(Dtype::F32, vec![512, 4], &bytes_pos)?,
        );

        let token_type_bytes = &bytes_layer[0..32]; 
        tensors.insert(
            "embeddings.token_type_embeddings.weight".to_string(),
            TensorView::new(Dtype::F32, vec![2, 4], token_type_bytes)?,
        );

        tensors.insert(
            "embeddings.LayerNorm.weight".to_string(),
            TensorView::new(Dtype::F32, shape_bias.clone(), &bytes_bias)?,
        );
        tensors.insert(
            "embeddings.LayerNorm.bias".to_string(),
            TensorView::new(Dtype::F32, shape_bias.clone(), &bytes_bias)?,
        );

        tensors.insert(
            "encoder.layer.0.attention.self.query.weight".to_string(),
            TensorView::new(Dtype::F32, shape_layer.clone(), &bytes_layer)?,
        );
        tensors.insert(
            "encoder.layer.0.attention.self.query.bias".to_string(),
            TensorView::new(Dtype::F32, shape_bias.clone(), &bytes_bias)?,
        );
        tensors.insert(
            "encoder.layer.0.attention.self.key.weight".to_string(),
            TensorView::new(Dtype::F32, shape_layer.clone(), &bytes_layer)?,
        );
        tensors.insert(
            "encoder.layer.0.attention.self.key.bias".to_string(),
            TensorView::new(Dtype::F32, shape_bias.clone(), &bytes_bias)?,
        );
        tensors.insert(
            "encoder.layer.0.attention.self.value.weight".to_string(),
            TensorView::new(Dtype::F32, shape_layer.clone(), &bytes_layer)?,
        );
        tensors.insert(
            "encoder.layer.0.attention.self.value.bias".to_string(),
            TensorView::new(Dtype::F32, shape_bias.clone(), &bytes_bias)?,
        );

        // Self Output
        tensors.insert(
            "encoder.layer.0.attention.output.dense.weight".to_string(),
            TensorView::new(Dtype::F32, shape_layer.clone(), &bytes_layer)?,
        );
        tensors.insert(
            "encoder.layer.0.attention.output.dense.bias".to_string(),
            TensorView::new(Dtype::F32, shape_bias.clone(), &bytes_bias)?,
        );
        tensors.insert(
            "encoder.layer.0.attention.output.LayerNorm.weight".to_string(),
            TensorView::new(Dtype::F32, shape_bias.clone(), &bytes_bias)?,
        );
        tensors.insert(
            "encoder.layer.0.attention.output.LayerNorm.bias".to_string(),
            TensorView::new(Dtype::F32, shape_bias.clone(), &bytes_bias)?,
        );

        // Intermediate
        tensors.insert(
            "encoder.layer.0.intermediate.dense.weight".to_string(),
            TensorView::new(Dtype::F32, shape_layer.clone(), &bytes_layer)?,
        );
        tensors.insert(
            "encoder.layer.0.intermediate.dense.bias".to_string(),
            TensorView::new(Dtype::F32, shape_bias.clone(), &bytes_bias)?,
        );

        // Output
        tensors.insert(
            "encoder.layer.0.output.dense.weight".to_string(),
            TensorView::new(Dtype::F32, shape_layer.clone(), &bytes_layer)?,
        );
        tensors.insert(
            "encoder.layer.0.output.dense.bias".to_string(),
            TensorView::new(Dtype::F32, shape_bias.clone(), &bytes_bias)?,
        );
        tensors.insert(
            "encoder.layer.0.output.LayerNorm.weight".to_string(),
            TensorView::new(Dtype::F32, shape_bias.clone(), &bytes_bias)?,
        );
        tensors.insert(
            "encoder.layer.0.output.LayerNorm.bias".to_string(),
            TensorView::new(Dtype::F32, shape_bias.clone(), &bytes_bias)?,
        );

        safetensors::serialize_to_file(&tensors, &None, &path.join("model.safetensors"))?;

        // 2. Create Config
        let config_json = r#"{
            "vocab_size": 10,
            "hidden_size": 4,
            "num_hidden_layers": 1,
            "num_attention_heads": 2,
            "intermediate_size": 4,
            "hidden_act": "gelu",
            "type_vocab_size": 2,
            "max_position_embeddings": 512,
            "layer_norm_eps": 1e-12,
            "model_type": "bert"
        }"#;
        std::fs::write(path.join("config.json"), config_json)?;

        // 3. Create Tokenizer
        let tokenizer_json = r#"{
          "version": "1.0",
          "truncation": null,
          "padding": null,
          "added_tokens": [
             { 
                 "id": 0, "content": "[PAD]", "single_word": false, "lstrip": false, "rstrip": false, "normalized": false, "special": true 
             },
             { 
                 "id": 1, "content": "[CLS]", "single_word": false, "lstrip": false, "rstrip": false, "normalized": false, "special": true 
             },
             { 
                 "id": 2, "content": "[SEP]", "single_word": false, "lstrip": false, "rstrip": false, "normalized": false, "special": true 
             }
          ],
          "model": {
            "type": "WordLevel",
            "vocab": {
              "[PAD]": 0, "[CLS]": 1, "[SEP]": 2, "hello": 3, "world": 4
            },
            "unk_token": "[UNK]"
          }
        }"#;
        std::fs::write(path.join("tokenizer.json"), tokenizer_json)?;

        Ok(dir)
    }

    #[test]
    fn test_load_minilm_bert_config() -> Result<()> {
        let dir = create_dummy_bert_files()?;

        // Try loading as MiniLM (which uses BERT architecture)
        let encoder = SentenceEncoder::from_pretrained(
            dir.path(),
            Device::Cpu,
            None,
            None,
            Some(ModelType::MiniLML6V2),
        );

        assert!(
            encoder.is_ok(),
            "Failed to load SentenceEncoder: {:?}",
            encoder.err()
        );
        let encoder = encoder.unwrap();

        // Verify config loaded values
        assert_eq!(encoder.hidden_size(), 4);
        assert_eq!(encoder.vocab_size(), 10);

        // Verify Tokenizer loaded
        assert_eq!(encoder.tokenizer().get_vocab_size(true), 5);

        Ok(())
    }

    const TOLERANCE: f32 = 1e-3;

    #[tokio::test]
    async fn test_torch_sentence_encoder_golden_values() -> Result<()> {
        let cpu_encoder =
            SentenceEncoder::from_registry(ModelType::MiniLML6V2, None, Device::Cpu, None, None)
                .await?;
        let context = WgpuContext::new().await?;
        let gpu_encoder = SentenceEncoder::from_registry(
            ModelType::MiniLML6V2,
            None,
            Device::Wgpu,
            Some(context),
            None,
        )
        .await?;
        let sentence = "ILoveEdgeGPT";
        let output_1 = cpu_encoder.encode(sentence).await?;
        for (i, &val) in output_1.iter().enumerate() {
            let expected = DATA[0][i];
            let diff = (val - expected).abs();
            assert!(
                diff < TOLERANCE,
                "Value mismatch at index {}: got {}, expected {}, diff {}",
                i,
                val,
                expected,
                diff
            );
        }
        let output_2 = gpu_encoder.encode(sentence).await?;
        for (i, &val) in output_2.iter().enumerate() {
            let expected = DATA[0][i];
            let diff = (val - expected).abs();
            assert!(
                diff < TOLERANCE,
                "Value mismatch at index {}: got {}, expected {}, diff {}",
                i,
                val,
                expected,
                diff
            );
        }
        Ok(())
    }

    #[tokio::test]
    async fn test_torch_sentence_encoder_cls_golden_values() -> Result<()> {
        let cpu_encoder =
            SentenceEncoder::from_registry(ModelType::MiniLML6V2, None, Device::Cpu, None, None)
                .await?;
        let context = WgpuContext::new().await?;
        let gpu_encoder = SentenceEncoder::from_registry(
            ModelType::MiniLML6V2,
            None,
            Device::Wgpu,
            Some(context),
            None,
        )
        .await?;
        let sentence = "ILoveEdgeGPT";
        let output_1 = cpu_encoder
            .encode_with(sentence, Some("cls"), false)
            .await?;
        for (i, &val) in output_1.iter().enumerate() {
            let expected = CLS_DATA[0][i];
            let diff = (val - expected).abs();
            assert!(
                diff < TOLERANCE,
                "Value mismatch at index {}: got {}, expected {}, diff {}",
                i,
                val,
                expected,
                diff
            );
        }
        let output_2 = gpu_encoder
            .encode_with(sentence, Some("cls"), false)
            .await?;
        for (i, &val) in output_2.iter().enumerate() {
            let expected = CLS_DATA[0][i];
            let diff = (val - expected).abs();
            assert!(
                diff < TOLERANCE,
                "Value mismatch at index {}: got {}, expected {}, diff {}",
                i,
                val,
                expected,
                diff
            );
        }
        Ok(())
    }
    #[tokio::test]
    async fn test_encode_raw_unnormalized() -> Result<()> {
        let encoder =
            SentenceEncoder::from_registry(ModelType::MiniLML6V2, None, Device::Cpu, None, None)
                .await?;
        let text = "This is a test sentence.";
        let normalized = encoder.encode(text).await?;
        let raw = encoder.encode_raw(text).await?;
        assert_eq!(normalized.len(), raw.len());
        let raw_norm: f32 = raw.iter().map(|x| x * x).sum::<f32>().sqrt();
        assert!(
            (raw_norm - 1.0).abs() > 0.01,
            "Raw embedding should not be L2-normalized, but norm is {:.6}",
            raw_norm
        );
        let normalized_norm: f32 = normalized.iter().map(|x| x * x).sum::<f32>().sqrt();
        assert!(
            (normalized_norm - 1.0).abs() < 0.001,
            "Normalized embedding should have L2 norm ~1.0, but got {:.6}",
            normalized_norm
        );
        Ok(())
    }
    #[tokio::test]
    async fn test_encode_batch_raw() -> Result<()> {
        let encoder =
            SentenceEncoder::from_registry(ModelType::MiniLML6V2, None, Device::Cpu, None, None)
                .await?;
        let texts = vec!["First sentence", "Second sentence", "Third sentence"];
        let normalized_batch = encoder.encode_batch(&texts).await?;
        let raw_batch = encoder.encode_batch_raw(&texts).await?;
        assert_eq!(normalized_batch.len(), raw_batch.len());
        assert_eq!(normalized_batch.len(), 3);
        for (i, (norm_emb, raw_emb)) in normalized_batch.iter().zip(raw_batch.iter()).enumerate() {
            assert_eq!(norm_emb.len(), raw_emb.len());
            let norm: f32 = norm_emb.iter().map(|x| x * x).sum::<f32>().sqrt();
            let raw_norm: f32 = raw_emb.iter().map(|x| x * x).sum::<f32>().sqrt();
            assert!(
                (norm - 1.0).abs() < 0.001,
                "Normalized embedding {} should have norm ~1.0",
                i
            );
            assert!(
                (raw_norm - 1.0).abs() > 0.01,
                "Raw embedding {} should not be normalized",
                i
            );
        }
        Ok(())
    }

    #[tokio::test]
    async fn test_encode_batch_with_full_control() -> Result<()> {
        let encoder =
            SentenceEncoder::from_registry(ModelType::MiniLML6V2, None, Device::Cpu, None, None)
                .await?;
        let texts = vec!["Sentence one", "Sentence two"];
        let mean_norm = encoder
            .encode_batch_with(&texts, Some("mean"), true)
            .await?;
        let mean_raw = encoder
            .encode_batch_with(&texts, Some("mean"), false)
            .await?;
        let cls_norm = encoder.encode_batch_with(&texts, Some("cls"), true).await?;
        let cls_raw = encoder
            .encode_batch_with(&texts, Some("cls"), false)
            .await?;
        assert_eq!(mean_norm.len(), 2);
        assert_eq!(mean_raw.len(), 2);
        assert_eq!(cls_norm.len(), 2);
        assert_eq!(cls_raw.len(), 2);
        let mean_norm_l2: f32 = mean_norm[0].iter().map(|x| x * x).sum::<f32>().sqrt();
        let mean_raw_l2: f32 = mean_raw[0].iter().map(|x| x * x).sum::<f32>().sqrt();
        assert!((mean_norm_l2 - 1.0).abs() < 0.001);
        assert!((mean_raw_l2 - 1.0).abs() > 0.01);
        Ok(())
    }
    #[tokio::test]
    async fn test_encode_with_custom_pooling() -> Result<()> {
        let encoder =
            SentenceEncoder::from_registry(ModelType::MiniLML6V2, None, Device::Cpu, None, None)
                .await?;
        let text = "This is a test sentence.";
        let mean_embedding = encoder.encode_with(text, Some("mean"), true).await?;
        let cls_embedding = encoder.encode_with(text, Some("cls"), true).await?;
        let mean_norm: f32 = mean_embedding.iter().map(|x| x * x).sum::<f32>().sqrt();
        let cls_norm: f32 = cls_embedding.iter().map(|x| x * x).sum::<f32>().sqrt();
        assert!(
            (mean_norm - 1.0).abs() < 0.001,
            "Mean pooled embedding should be normalized"
        );
        assert!(
            (cls_norm - 1.0).abs() < 0.001,
            "CLS pooled embedding should be normalized"
        );
        let dot_product: f32 = mean_embedding
            .iter()
            .zip(cls_embedding.iter())
            .map(|(a, b)| a * b)
            .sum();
        assert!(
            dot_product < 0.99,
            "Mean and CLS pooling should produce different embeddings, but similarity is {:.4}",
            dot_product
        );
        Ok(())
    }

    const CLS_DATA: [[f32; 384]; 1] = [[
        -0.5163895487785339,
        -0.1574437916278839,
        0.3061314523220062,
        -0.30399590730667114,
        -0.001997139770537615,
        -0.4957883059978485,
        0.15313491225242615,
        0.5460152626037598,
        -0.361857533454895,
        0.11269077658653259,
        0.1990087479352951,
        0.02300017699599266,
        -0.40553900599479675,
        -0.009781910106539726,
        -0.36328747868537903,
        0.43005451560020447,
        -0.052345480769872665,
        0.1001029759645462,
        -0.08436750620603561,
        0.11834143847227097,
        -0.18073385953903198,
        0.0609293170273304,
        0.02690977230668068,
        -0.3928368091583252,
        -0.1958770751953125,
        0.592823326587677,
        -0.16542725265026093,
        -0.2885848581790924,
        -0.005406714044511318,
        -0.704167366027832,
        0.049988847225904465,
        0.03190472722053528,
        0.1279837191104889,
        -0.06680043041706085,
        -0.24501022696495056,
        -0.1271894872188568,
        -0.37712928652763367,
        -0.0909680426120758,
        0.23225189745426178,
        0.07340285181999207,
        -0.04167001321911812,
        0.06211327016353607,
        0.08342063426971436,
        0.13633888959884644,
        -0.00837214756757021,
        -0.1006491407752037,
        -0.33831045031547546,
        0.25941598415374756,
        -0.11882565915584564,
        0.3253571689128876,
        -0.23151136934757233,
        0.07129789888858795,
        -0.3538666367530823,
        -0.38354235887527466,
        -0.04278576746582985,
        0.1854325383901596,
        -0.09360237419605255,
        0.4196222722530365,
        -0.040254440158605576,
        0.6520864963531494,
        0.5664708018302917,
        0.21036770939826965,
        -0.5855899453163147,
        0.1300504058599472,
        -0.5732654333114624,
        0.005646999925374985,
        0.12714526057243347,
        0.2289549559354782,
        0.0049773044884204865,
        -0.0029733169358223677,
        0.2160186469554901,
        0.3001004159450531,
        -0.5746078491210938,
        0.024782760068774223,
        0.5630379915237427,
        0.021069854497909546,
        -0.030697371810674667,
        0.05293728783726692,
        -0.004391330759972334,
        -0.1632883995771408,
        -0.09198983758687973,
        0.14928650856018066,
        0.17426720261573792,
        0.08451197296380997,
        -0.025981053709983826,
        -0.28240081667900085,
        0.04700728505849838,
        0.28292378783226013,
        0.22831608355045319,
        0.2947469651699066,
        0.01462653186172247,
        0.2790127396583557,
        -0.05447155609726906,
        -0.1932484656572342,
        -0.5073180794715881,
        -0.19544652104377747,
        -0.16105243563652039,
        -0.39773738384246826,
        -0.37824469804763794,
        5.820425510406494,
        -0.11012030392885208,
        0.2667139768600464,
        0.0029075888451188803,
        0.05947202071547508,
        0.039904333651065826,
        0.1782597005367279,
        0.0027309353463351727,
        -0.42010921239852905,
        0.08425631374120712,
        -0.12917421758174896,
        0.02220550924539566,
        -0.5403255820274353,
        0.16335909068584442,
        -0.309167742729187,
        -0.02415195293724537,
        -0.2842243015766144,
        0.029768727719783783,
        0.05242712423205376,
        0.6045957207679749,
        0.04920327290892601,
        -0.09456705302000046,
        -0.30333763360977173,
        -0.08245895802974701,
        -0.06620944291353226,
        -0.4188319444656372,
        -0.9791956543922424,
        0.04602351784706116,
        -4.5443215703248034e-32,
        0.10802309215068817,
        0.052593182772397995,
        -0.23095546662807465,
        -0.42161819338798523,
        -0.06458849459886551,
        0.20455586910247803,
        -0.22190947830677032,
        -0.09158625453710556,
        -0.1730123907327652,
        -0.15389271080493927,
        -0.43525460362434387,
        -0.6433721780776978,
        -0.1847332864999771,
        0.058133065700531006,
        0.0925348773598671,
        -0.15584421157836914,
        0.056978534907102585,
        0.16792336106300354,
        -0.46124985814094543,
        0.2738616168498993,
        0.29099488258361816,
        0.07888606935739517,
        0.006522865034639835,
        -0.06756965816020966,
        0.08734457939863205,
        0.17999905347824097,
        0.25664758682250977,
        -0.11871721595525742,
        0.017898427322506905,
        -0.221653014421463,
        -0.27748700976371765,
        -0.13043370842933655,
        0.0067644803784787655,
        -0.051287077367305756,
        -0.4264832139015198,
        0.2979823052883148,
        -0.21642445027828217,
        -0.35859689116477966,
        0.2375723272562027,
        -0.014049279503524303,
        -0.08437429368495941,
        0.08803442120552063,
        0.0826771929860115,
        -0.4135761260986328,
        -0.22375692427158356,
        0.21956419944763184,
        -0.012833942659199238,
        0.2605896294116974,
        -0.11175251007080078,
        0.09771503508090973,
        -0.03880155086517334,
        0.5077845454216003,
        -0.09225456416606903,
        0.32548463344573975,
        -0.0007880323100835085,
        0.026695769280195236,
        -0.06863802671432495,
        0.7305068373680115,
        0.17433954775333405,
        -0.0660107210278511,
        0.2899642884731293,
        -0.5798450112342834,
        -0.23316612839698792,
        0.5817927122116089,
        0.013148696161806583,
        0.35609200596809387,
        0.060257017612457275,
        0.026319583877921104,
        0.1289019137620926,
        -0.13503329455852509,
        -0.6037858128547668,
        -0.22703108191490173,
        -0.1781323254108429,
        -0.15026666224002838,
        -0.11201360076665878,
        0.05352163314819336,
        -0.11069157719612122,
        0.39073655009269714,
        0.029580064117908478,
        0.03409364074468613,
        -0.4953951835632324,
        0.23437267541885376,
        0.2971281409263611,
        -0.188616082072258,
        0.2591162621974945,
        0.027757804840803146,
        0.26229342818260193,
        0.09256341308355331,
        -0.11861276626586914,
        0.46081840991973877,
        -0.3648015856742859,
        0.10114793479442596,
        -0.008298822678625584,
        0.3080815374851227,
        -0.008283806033432484,
        3.72298074526543e-32,
        -0.18549823760986328,
        0.1989746242761612,
        0.03534294664859772,
        0.11167028546333313,
        -0.16271910071372986,
        0.48354825377464294,
        0.16083939373493195,
        0.25778427720069885,
        -0.05402795970439911,
        0.014940399676561356,
        -0.06705246865749359,
        0.4935746192932129,
        -0.2005692571401596,
        0.06228955462574959,
        0.24781356751918793,
        -0.39983269572257996,
        0.11253201961517334,
        0.4341815114021301,
        -0.05290815606713295,
        0.2618178129196167,
        0.014166053384542465,
        -0.2553460896015167,
        -0.12818093597888947,
        0.18345282971858978,
        -0.27936750650405884,
        0.25215214490890503,
        0.273010790348053,
        -0.3841612637042999,
        -0.06705249845981598,
        -0.03211165592074394,
        0.3189519941806793,
        0.12854735553264618,
        -0.17446698248386383,
        -0.3210836350917816,
        0.016429398208856583,
        0.5586832165718079,
        0.46830877661705017,
        -0.10660684108734131,
        -0.45169225335121155,
        0.08698295801877975,
        0.18824830651283264,
        0.3258715569972992,
        0.18276727199554443,
        0.06596913188695908,
        -0.3994021713733673,
        0.20355726778507233,
        0.24678397178649902,
        0.16835372149944305,
        -0.21549256145954132,
        -0.06255121529102325,
        -0.060728251934051514,
        0.476147323846817,
        0.44310837984085083,
        0.2010100781917572,
        0.02157735824584961,
        -0.14610250294208527,
        0.008000519126653671,
        -0.21238677203655243,
        -0.42586931586265564,
        -0.17036700248718262,
        -0.2819638252258301,
        -0.14009398221969604,
        -0.056138843297958374,
        -0.09069202840328217,
        -0.12535466253757477,
        -0.07296551764011383,
        0.10725443810224533,
        -0.02452963963150978,
        -0.5348902344703674,
        0.036405134946107864,
        -0.13416391611099243,
        0.14536866545677185,
        -0.3260209560394287,
        -0.05832880735397339,
        0.13395293056964874,
        0.0024890578351914883,
        -0.10055498778820038,
        -0.10344457626342773,
        0.10878889262676239,
        -0.2185640037059784,
        0.4241520166397095,
        -0.12317123264074326,
        0.1508142352104187,
        -0.1291942298412323,
        -0.14540773630142212,
        -0.2309550791978836,
        -0.12470992654561996,
        0.10720044374465942,
        0.30971458554267883,
        0.3771718144416809,
        -0.19559745490550995,
        0.0655628889799118,
        -0.3361065685749054,
        0.005755702033638954,
        0.15620873868465424,
        -8.821881891662997e-08,
        -0.18669633567333221,
        0.3872677981853485,
        -0.008671572431921959,
        0.026022493839263916,
        -0.19233280420303345,
        0.01227121613919735,
        -0.010159069672226906,
        0.03858240693807602,
        -0.17931485176086426,
        -0.3388744592666626,
        -0.21717920899391174,
        0.04711611196398735,
        -0.2705745995044708,
        0.25901976227760315,
        -0.03811793401837349,
        -0.33722302317619324,
        0.04094010218977928,
        0.14641958475112915,
        0.09261398017406464,
        0.20519708096981049,
        -0.04376998916268349,
        -0.034352220594882965,
        0.15865451097488403,
        -0.22382010519504547,
        -0.18155543506145477,
        -0.3642354905605316,
        0.22201648354530334,
        -0.36855989694595337,
        0.5264513492584229,
        0.26681312918663025,
        -0.04413700848817825,
        0.3405328392982483,
        0.1832791268825531,
        0.03924543410539627,
        -0.25244078040122986,
        -0.23242086172103882,
        -0.06784724444150925,
        -0.01902410201728344,
        0.25132080912590027,
        -0.15016235411167145,
        -0.25774475932121277,
        0.18054689466953278,
        0.1611836850643158,
        -0.07434374839067459,
        0.04055296629667282,
        0.007087182253599167,
        0.2825707495212555,
        -0.20020878314971924,
        -0.05281022936105728,
        -0.24009709060192108,
        -0.29153335094451904,
        0.13965345919132233,
        0.2204449623823166,
        0.13636744022369385,
        0.545246422290802,
        0.03615644946694374,
        -0.04260753467679024,
        -0.008099211379885674,
        0.11791657656431198,
        -0.17479459941387177,
        -0.12719477713108063,
        -0.2938424348831177,
        -0.11166748404502869,
        -0.08005636930465698,
    ]];

    // torch output
    const DATA: [[f32; 384]; 1] = [[
        -0.11821649968624115,
        0.01483845990151167,
        0.032413460314273834,
        -0.03832545876502991,
        -0.08420868963003159,
        -0.07704762369394302,
        0.09857901930809021,
        0.15451136231422424,
        -0.012817851267755032,
        -0.038907185196876526,
        0.036726806312799454,
        -0.0587584413588047,
        -0.09905494749546051,
        0.024553759023547173,
        -0.03212520852684975,
        0.023764265701174736,
        0.002320743165910244,
        0.02741330862045288,
        -0.056977882981300354,
        -0.058989036828279495,
        -0.05894921347498894,
        0.018924027681350708,
        0.016301238909363747,
        -0.03757140412926674,
        0.011620789766311646,
        0.10398710519075394,
        -0.023662973195314407,
        -0.0487462654709816,
        0.036607515066862106,
        -0.03529171645641327,
        0.036428939551115036,
        0.055430516600608826,
        0.05849788337945938,
        0.015926543623209,
        -0.02163754031062126,
        -0.01850154809653759,
        -0.07779848575592041,
        -0.02604863978922367,
        0.05127520114183426,
        0.03162091225385666,
        -0.029950276017189026,
        -0.022274397313594818,
        0.0017825353424996138,
        -0.014800788834691048,
        0.04142190143465996,
        0.0015077805146574974,
        -0.06543765962123871,
        0.03164982423186302,
        -0.03673189878463745,
        0.08233001828193665,
        0.004210473503917456,
        -0.017630603164434433,
        -0.04623855650424957,
        -0.04396460950374603,
        0.007714977953583002,
        0.019786227494478226,
        -0.006975528318434954,
        0.09412596374750137,
        0.00685233436524868,
        0.04973898455500603,
        0.10995630919933319,
        -0.002000414300709963,
        -0.09676433354616165,
        0.03507264703512192,
        -0.1115514487028122,
        0.0062322914600372314,
        0.051586613059043884,
        0.036167971789836884,
        0.04377090930938721,
        0.0033521130681037903,
        0.03936653211712837,
        0.013658908195793629,
        -0.04732489958405495,
        0.02136286161839962,
        0.06866159290075302,
        0.005695185158401728,
        0.03198903799057007,
        -0.04265598580241203,
        0.0004827573720831424,
        -0.05892489105463028,
        0.04001970961689949,
        -0.007046851795166731,
        0.014431517571210861,
        -0.003059250069782138,
        -0.0035280915908515453,
        -0.0307855773717165,
        0.01691993698477745,
        0.0596936009824276,
        0.07663606852293015,
        0.05313553661108017,
        0.0011810563737526536,
        0.05649613216519356,
        -0.10515657812356949,
        -0.02123952843248844,
        -0.0111449109390378,
        -0.024368705227971077,
        -0.05183457210659981,
        -0.09021260589361191,
        -0.16594848036766052,
        0.14390702545642853,
        -0.01965988799929619,
        0.02318192459642887,
        -0.027438664808869362,
        0.065290667116642,
        0.008947581984102726,
        0.035180266946554184,
        -0.010472030378878117,
        -0.06960415095090866,
        0.03070148639380932,
        0.012879075482487679,
        -0.0648479014635086,
        -0.05852225795388222,
        0.019064677879214287,
        -0.06339516490697861,
        0.0022368181962519884,
        -0.002518467837944627,
        -0.005964819807559252,
        -0.00036860068212263286,
        0.07726074010133743,
        -0.011329378932714462,
        0.0008099133847281337,
        -0.03023538738489151,
        -0.051249127835035324,
        -0.008752761408686638,
        -0.05868353322148323,
        -0.04001897946000099,
        0.0779682993888855,
        8.161184067158072e-35,
        -0.007281572557985783,
        -0.01890953816473484,
        -0.029203590005636215,
        -0.07203167676925659,
        -0.009677587077021599,
        0.05291109159588814,
        -0.05385264381766319,
        -0.02590499073266983,
        -0.054628562182188034,
        0.014772310853004456,
        -0.06659639626741409,
        -0.12317263334989548,
        -0.06387009471654892,
        0.05377579480409622,
        0.08643139153718948,
        -0.035996779799461365,
        -0.01683029718697071,
        0.05442662909626961,
        -0.04740486294031143,
        0.019947156310081482,
        0.018279142677783966,
        -0.018131552264094353,
        0.00030245358357205987,
        -0.018308790400624275,
        0.02609851025044918,
        -0.012348663061857224,
        0.08222058415412903,
        -0.059184253215789795,
        0.07634028047323227,
        0.05468760058283806,
        -0.014903590083122253,
        -0.023302767425775528,
        -0.02637770026922226,
        0.008994209580123425,
        -0.06773535907268524,
        0.10640077292919159,
        0.002032886492088437,
        -0.12646766006946564,
        0.029074354097247124,
        -0.007879190146923065,
        0.002665302949026227,
        0.01898948848247528,
        -0.005210061091929674,
        -0.06717012077569962,
        0.006390294060111046,
        0.04278677701950073,
        -3.102064738413901e-06,
        0.025280017405748367,
        0.03275349736213684,
        0.05921081453561783,
        0.0031939004547894,
        0.04699588567018509,
        0.011941044591367245,
        0.028008639812469482,
        -0.04421579837799072,
        0.016509806737303734,
        -0.0438409149646759,
        0.0883801206946373,
        0.057739563286304474,
        0.012457718141376972,
        0.043354082852602005,
        -0.06048839911818504,
        -0.04233165085315704,
        0.03582974895834923,
        -0.028236007317900658,
        0.04259493201971054,
        -0.02610466443002224,
        0.013341291807591915,
        0.007853543385863304,
        -0.042704347521066666,
        -0.11722038686275482,
        -0.04111486300826073,
        -0.0020551832858473063,
        0.029318220913410187,
        -0.02152344025671482,
        0.02934369072318077,
        -0.017047198489308357,
        0.07452324032783508,
        -0.024426523596048355,
        -0.04400409385561943,
        -0.1752942055463791,
        0.002598874270915985,
        0.07134081423282623,
        -0.07909136265516281,
        0.109341561794281,
        0.02022448368370533,
        0.02596624568104744,
        -0.05930293723940849,
        0.010439978912472725,
        0.05578886345028877,
        -0.11619339138269424,
        0.04506910592317581,
        -0.02036040835082531,
        0.07437614351511002,
        0.02605449967086315,
        -9.264142610517451e-34,
        -0.08870042860507965,
        0.013286168687045574,
        0.004052808973938227,
        0.05999457463622093,
        -0.03495628014206886,
        0.03324523940682411,
        -0.033129166811704636,
        0.08669110387563705,
        -0.030751707032322884,
        0.048548534512519836,
        0.028526773676276207,
        0.021484967321157455,
        0.030193649232387543,
        -0.009923255071043968,
        -0.0001951794547494501,
        -0.04323520138859749,
        0.0933508574962616,
        0.001316147274337709,
        -0.05098430812358856,
        0.070722296833992,
        -0.050041988492012024,
        -0.05217379331588745,
        -0.07135564833879471,
        0.016833757981657982,
        -0.025882845744490623,
        0.050928995013237,
        0.07698167860507965,
        -0.08845169842243195,
        -0.051101621240377426,
        -0.00917922705411911,
        0.0592721588909626,
        0.006766619626432657,
        -0.09075727313756943,
        -0.06669987738132477,
        -0.01072009652853012,
        0.12083634734153748,
        0.1569482386112213,
        0.0012705506524071097,
        -0.12961065769195557,
        0.0550946444272995,
        0.04608116298913956,
        0.040396757423877716,
        0.008258886635303497,
        0.07624633610248566,
        -0.0334036760032177,
        0.04454505816102028,
        -0.042472075670957565,
        0.05451638996601105,
        -0.03309880569577217,
        0.02402544394135475,
        -0.0021244243253022432,
        0.05715480446815491,
        0.063016377389431,
        0.03535502031445503,
        0.02551184594631195,
        -0.014471441507339478,
        0.03163847699761391,
        -0.060215868055820465,
        -0.09598607569932938,
        -0.008401725441217422,
        -0.01361356396228075,
        -0.025888152420520782,
        -0.03794090449810028,
        0.01725318655371666,
        0.08508910983800888,
        -0.019669588655233383,
        0.011736869812011719,
        -0.032496415078639984,
        -0.015824109315872192,
        0.03101789578795433,
        0.003995380364358425,
        0.055204860866069794,
        -0.05612315237522125,
        -0.036251939833164215,
        -0.004520803689956665,
        0.008062152191996574,
        0.04662007465958595,
        -0.0030310347210615873,
        0.0009401417337357998,
        -0.03156513720750809,
        0.07167726010084152,
        -0.047081559896469116,
        0.039878182113170624,
        -0.01845593750476837,
        0.05011848360300064,
        -0.06033334881067276,
        0.0273140762001276,
        0.02804018370807171,
        0.03279770910739899,
        0.04144733026623726,
        -0.04231937229633331,
        0.05296974256634712,
        -0.04298911243677139,
        0.03831182420253754,
        0.025227583944797516,
        -1.7168249044630102e-08,
        0.009895655326545238,
        0.059838224202394485,
        -0.021619761362671852,
        0.0030213319696485996,
        -0.014637143351137638,
        -0.019520234316587448,
        -0.01953577809035778,
        0.023791272193193436,
        -0.06648516654968262,
        -0.01364162564277649,
        -0.02240905538201332,
        0.03731020912528038,
        -0.10485932976007462,
        0.08963263779878616,
        0.009284150786697865,
        -0.059183165431022644,
        -0.010174667462706566,
        0.02015499584376812,
        -0.022829821333289146,
        0.0287936981767416,
        -0.06580676138401031,
        -0.06072871387004852,
        0.07623159140348434,
        -0.08871720731258392,
        -0.01628008298575878,
        -0.030458327382802963,
        0.022412847727537155,
        0.021820131689310074,
        0.08670350164175034,
        0.05940425023436546,
        0.008340534754097462,
        0.10029196739196777,
        0.03987852856516838,
        -0.04734252020716667,
        -0.02639825828373432,
        0.026036174967885017,
        -0.021857457235455513,
        0.001385273179039359,
        0.025563646107912064,
        -0.03491564840078354,
        -0.009994474239647388,
        0.02015668898820877,
        0.049174755811691284,
        -0.02666422724723816,
        -0.01904996857047081,
        -0.037382833659648895,
        0.06344708800315857,
        -0.028231235221028328,
        -0.0010251261992380023,
        -0.027988407760858536,
        -0.0947716161608696,
        0.04263103008270264,
        0.0327896922826767,
        0.019741468131542206,
        0.09609098732471466,
        0.03758559748530388,
        0.022560931742191315,
        0.00798097625374794,
        -0.026785042136907578,
        0.014180107973515987,
        -0.032463762909173965,
        -0.041690416634082794,
        0.04956276714801788,
        -0.01912812329828739,
    ]];
}
