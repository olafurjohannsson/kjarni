# TODO: Reusable Components for Decoder Models

## P0 - Production Hardening âœ… NEW

### 1. Cancellation Tokens âœ…
- [x] Create `kjarni-transformers/src/common/cancellation.rs`
- [x] `CancellationToken` + `CancellationHandle` pair
- [x] `generate_cancellable()` methods in `DecoderGenerator`
- [x] Cancellation checks in generation loop

### 2. GPU Timeout Handling âœ…
- [x] Create `kjarni-transformers/src/gpu_ops/timeout.rs`
- [x] `GpuTimeoutConfig` with configurable timeout/poll interval
- [x] `poll_with_timeout_async()` for buffer mapping
- [x] Timeout-aware `read_logits_from_staging()`

### 3. Generation Statistics âœ…
- [x] Create `kjarni-transformers/src/stats.rs`
- [x] `GenerationStats` with prefill/decode TPS tracking
- [x] Global enable/disable via `GenerationStats::enable()`
- [x] Integrated into `DecoderGenerator`

## P1 - High Impact

### 1. LoadedRoPE âœ… (Verify)
- [x] Create `kjarni-transformers/src/rope/loader.rs`
- [x] `LoadedRoPE` struct with `cpu_rope: Arc<RoPE>`, `gpu_rope: Option<GpuRoPE>`
- [x] `new(ctx, head_dim, max_seq_len, theta, scaling, load_gpu) -> Result<Self>`
- [x] Used by: Llama, Mistral, Phi, Gemma, Qwen

### 2. DecoderPipeline Builder âœ… (Verify)
- [x] Add `DecoderPipelineBuilder` to `kjarni-transformers/src/pipeline/decoder.rs`
- [x] `DecoderPipeline::builder().embeddings(...).lm_head(...).plan_from_device(...).build()`
- [x] Auto-create execution plan from device + ModelLoadConfig
- [x] Reduce boilerplate in model `from_pretrained`

### 3. DecoderModelFactory Trait âœ… (Verify)
- [x] Create `kjarni-transformers/src/pipeline/loader.rs`
- [x] Trait with `load_config`, `build_backends`
- [x] `GenericLoader::load_from_pretrained<M: DecoderModelFactory>()`
- [x] Each model just implements model-specific parts

### 4. CpuLayerFactory âœ… (Verify)
- [x] Create `kjarni-transformers/src/pipeline/factory.rs`
- [x] `build_norm()`, `build_swiglu_ffn()`, `build_decoder_attention()`
- [x] Layout-driven weight loading
- [x] Used by: Llama layers

## P2 - Medium Impact

### 4. TokenizerLoader
- [ ] Create `kjarni-transformers/src/tokenizer/loader.rs`
- [ ] `load_tokenizer(path, max_seq_len, padding_config) -> Result<Tokenizer>`
- [ ] Handle truncation, padding, special tokens uniformly
- [ ] Used by: All models

### 5. AttentionMaskFactory
- [ ] Create `kjarni-transformers/src/attention/mask.rs`
- [ ] `MaskType` enum: `Full`, `SlidingWindow(usize)`, `Alibi`
- [ ] `create_cpu(seq_len, past_len) -> Array2<f32>`
- [ ] `create_gpu(ctx, seq_len, max_len) -> Result<GpuTensor>`
- [ ] Used by: All models (Mistral needs SlidingWindow)

### 6. Seq2SeqPipeline
- [ ] Create `kjarni-transformers/src/pipeline/seq2seq.rs`
- [ ] `Seq2SeqPipeline` with encoder + decoder backends
- [ ] `Seq2SeqPipelineBuilder` (similar pattern to decoder)
- [ ] `Seq2SeqModelFactory` trait
- [ ] Refactor BART to use it

## P3 - Nice to Have

### 7. Fix LoadedEmbeddings API
- [ ] Make context `Option<&Arc<WgpuContext>>`
- [ ] Add `load_cpu: bool, load_gpu: bool` params
- [ ] Add `has_cpu()`, `has_gpu()` methods
- [ ] Match LoadedLMHead API pattern

### 8. Batch Generation
- [ ] Add `generate_batch()` to `DecoderGenerator`
- [ ] Batch-aware KV cache
- [ ] Batch prefill/decode in backends
- [ ] ~2-3 days effort

### 9. Backpressure Handling
- [ ] Bounded channel between generation and stream yield
- [ ] `StreamConfig` with buffer size
- [ ] ~4-8 hours effort

## Documentation

### 10. Backend Documentation âœ…
- [x] `GpuDecoderBackend` - Full module docs + method docs
- [x] `CpuDecoderBackend` - Full module docs + method docs
- [x] `AnyDecoderBackend` - Type erasure explanation
- [x] `DecoderGenerator` - Usage examples + architecture diagram

### 11. README & Examples
- [ ] Main README with supported models, formats, performance
- [ ] Example: Basic chat generation
- [ ] Example: Streaming generation
- [ ] Example: Cancellation handling
- [ ] Example: Custom model loading




ğŸ† The Optimization Priority Matrix
Priority	Component	Optimization	Impact	Complexity	Why it fixes "Large Models"
1	Attention	Zero-Copy KV Update	ğŸ›‘ Critical	Medium	Currently, forward_2 copies the entire history every token. On 7B models, this copy takes longer than the math.
2	Kernels	AVX2 RoPE (In-Place)	ğŸ”¥ High	Low	Allocating k_rope (size Batch*Seq*Dim) every step creates allocation jitter.
3	Kernels	AVX2 SwiGLU Fusion	ğŸ”¥ High	Medium	Fusing Gate * Sigmoid(Gate) * Up into one loop keeps data in L1 cache.
4	Kernels	AVX2 RMSNorm	âš¡ Medium	Low	Scalar RMSNorm is bandwidth-bound. AVX2 makes it instant.
5	Logic	Pre-Scaling Q	âš¡ Medium	Very Low	Changes 
O
(
N
)
O(N)
 math to 
O
(
1
)
O(1)
 math in the attention loop.


 Pre-Launch Checklist
Documentation
ItemPriorityStatusREADME with hero exampleP0â–¢README feature matrixP0â–¢README installation instructionsP0â–¢CLI help text (--help for all commands)P0â–¢rustdoc for public APIP0â–¢CHANGELOG.mdP1â–¢CONTRIBUTING.mdP2â–¢LICENSE (MIT/Apache-2.0 dual)P0â–¢Examples directoryP1â–¢Architecture overview (for contributors)P2â–¢
Testing
ItemPriorityStatusUnit tests (85%+ coverage)P0â–¢Integration tests (model loading)P0â–¢CLI integration testsP1â–¢Cross-platform CI (Linux, macOS, Windows)P0â–¢GPU tests (if available in CI)P2â–¢Benchmark suiteP1â–¢
Build & Distribution
ItemPriorityStatuscargo publish ready (all crates)P0â–¢Crate names reserved on crates.ioP0â–¢Pre-built binaries (GitHub Releases)P1â–¢cargo install kjarni worksP0â–¢cargo binstall supportP2â–¢Homebrew formulaP2â–¢
CLI Completeness
CommandWorkingStdin/StdoutHelp Textkjarni model listâ–¢N/Aâ–¢kjarni model downloadâ–¢N/Aâ–¢kjarni model removeâ–¢N/Aâ–¢kjarni model infoâ–¢N/Aâ–¢kjarni encodeâ–¢â–¢â–¢kjarni rerankâ–¢â–¢â–¢kjarni searchâ–¢â–¢â–¢kjarni classifyâ–¢â–¢â–¢kjarni summarizeâ–¢â–¢â–¢kjarni translateâ–¢â–¢â–¢kjarni transcribeâ–¢â–¢â–¢kjarni chatâ–¢â–¢â–¢kjarni generateâ–¢â–¢â–¢
CLI Unix Philosophy
bash# All these should work
cat document.txt | kjarni summarize
cat query.txt | kjarni encode --format raw > embedding.vec
cat audio.mp3 | kjarni transcribe | kjarni translate --to spanish
cat texts.jsonl | kjarni classify --model sentiment-distilbert
kjarni search --index ./my_index --query "rust performance" --top-k 10
```

| Requirement | Status |
|-------------|--------|
| Stdin detection (TTY vs pipe) | â–¢ |
| JSON output (`--format json`) | â–¢ |
| JSONL output (`--format jsonl`) | â–¢ |
| Raw output (`--format raw`) | â–¢ |
| Quiet mode (`-q`) | â–¢ |
| Consistent exit codes | â–¢ |
| Stderr for progress, stdout for data | â–¢ |

### Model Registry

| Item | Priority | Status |
|------|----------|--------|
| All models have `cli_name` | P0 | â–¢ |
| Download with progress bar | P1 | â–¢ |
| GGUF download option | P0 | â–¢ |
| Cache directory configurable | P1 | â–¢ |
| `is_downloaded` check works | P0 | â–¢ |
| Model removal works | P0 | â–¢ |
| Disk usage shown in `info` | P1 | â–¢ |

### FFI Preparation

| Item | Priority | Status |
|------|----------|--------|
| C header generation (`cbindgen`) | P0 | â–¢ |
| Opaque handle pattern | P0 | â–¢ |
| Error handling across boundary | P0 | â–¢ |
| Memory management (free functions) | P0 | â–¢ |
| Thread safety documented | P0 | â–¢ |
| No panics across FFI | P0 | â–¢ |

### Package Names to Reserve

| Platform | Package Name | Registry |
|----------|--------------|----------|
| Rust | `kjarni` | crates.io |
| Rust | `kjarni-transformers` | crates.io |
| Rust | `kjarni-models` | crates.io |
| Rust | `kjarni-search` | crates.io |
| Python | `kjarni` | PyPI |
| Node.js | `kjarni` | npm |
| Node.js | `@kjarni/core` | npm |
| Swift | `Kjarni` | Swift Package Index |
| C# | `Kjarni` | NuGet |
| Go | `github.com/user/kjarni-go` | Go modules |
| Ruby | `kjarni` | RubyGems |

### Error Handling

| Item | Status |
|------|--------|
| Consistent error types | â–¢ |
| Helpful error messages | â–¢ |
| Model not found â†’ suggest similar | â–¢ |
| Network errors handled gracefully | â–¢ |
| GPU not available â†’ fallback message | â–¢ |

### Performance

| Item | Status |
|------|--------|
| Benchmarks documented | â–¢ |
| Memory usage reasonable | â–¢ |
| No memory leaks (valgrind/ASAN) | â–¢ |
| Startup time acceptable | â–¢ |

---

## Folder Layout (kjarni-transformers)
```
kjarni-transformers/src/
â”‚
â”œâ”€â”€ lib.rs                          # Public exports, prelude
â”‚
â”œâ”€â”€ common/                         # Shared utilities (device-agnostic)
â”‚   â”œâ”€â”€ mod.rs
â”‚   â”œâ”€â”€ sampling.rs                 # SamplingConfig, top-k, top-p, temperature
â”‚   â”œâ”€â”€ generation.rs               # GenerationConfig, BeamSearchParams
â”‚   â””â”€â”€ tokenization.rs             # Tokenizer utilities
â”‚
â”œâ”€â”€ tensor/                         # Tensor types and conversions
â”‚   â”œâ”€â”€ mod.rs
â”‚   â”œâ”€â”€ dtype.rs                    # DType enum (F32, BF16, Q4_K, etc.)
â”‚   â””â”€â”€ convert.rs                  # Dtype conversions
â”‚
â”œâ”€â”€ weights/                        # Weight loading
â”‚   â”œâ”€â”€ mod.rs
â”‚   â”œâ”€â”€ model_weights.rs            # ModelWeights struct
â”‚   â”œâ”€â”€ safetensors.rs              # SafeTensors loader
â”‚   â””â”€â”€ gguf.rs                     # GGUF loader
â”‚
â”œâ”€â”€ traits/                         # Core traits (device-agnostic)
â”‚   â”œâ”€â”€ mod.rs
â”‚   â”œâ”€â”€ model_config.rs             # ModelConfig, ModelMetadata, ModelLayout
â”‚   â”œâ”€â”€ inference_model.rs          # InferenceModel trait
â”‚   â””â”€â”€ language_model.rs           # LanguageModel trait
â”‚
â”‚
â”‚ â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
â”‚                              CPU BACKEND
â”‚ â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
â”‚
â”œâ”€â”€ cpu/
â”‚   â”‚
â”‚   â”œâ”€â”€ mod.rs                      # CPU exports
â”‚   â”‚
â”‚   â”œâ”€â”€ kernels/                    # Low-level SIMD kernels
â”‚   â”‚   â”œâ”€â”€ mod.rs                  # Runtime dispatch
â”‚   â”‚   â”œâ”€â”€ scalar/                 # Fallback implementations
â”‚   â”‚   â”‚   â”œâ”€â”€ mod.rs
â”‚   â”‚   â”‚   â”œâ”€â”€ matmul.rs
â”‚   â”‚   â”‚   â”œâ”€â”€ softmax.rs
â”‚   â”‚   â”‚   â””â”€â”€ rope.rs
â”‚   â”‚   â”œâ”€â”€ x86/                    # x86_64 AVX2/AVX-512
â”‚   â”‚   â”‚   â”œâ”€â”€ mod.rs
â”‚   â”‚   â”‚   â”œâ”€â”€ matmul_avx2.rs
â”‚   â”‚   â”‚   â”œâ”€â”€ softmax_avx2.rs
â”‚   â”‚   â”‚   â”œâ”€â”€ rope_avx2.rs
â”‚   â”‚   â”‚   â””â”€â”€ q4k_avx2.rs         # Quantized matmul
â”‚   â”‚   â””â”€â”€ aarch64/                # ARM NEON
â”‚   â”‚       â”œâ”€â”€ mod.rs
â”‚   â”‚       â”œâ”€â”€ matmul_neon.rs
â”‚   â”‚       â”œâ”€â”€ softmax_neon.rs
â”‚   â”‚       â””â”€â”€ rope_neon.rs
â”‚   â”‚
â”‚   â”œâ”€â”€ ops/                        # Mid-level operations (use kernels)
â”‚   â”‚   â”œâ”€â”€ mod.rs
â”‚   â”‚   â”œâ”€â”€ linear.rs               # LinearLayer
â”‚   â”‚   â”œâ”€â”€ layer_norm.rs
â”‚   â”‚   â”œâ”€â”€ rms_norm.rs
â”‚   â”‚   â”œâ”€â”€ rope.rs                 # RoPE using kernels
â”‚   â”‚   â”œâ”€â”€ attention.rs            # Attention utilities
â”‚   â”‚   â””â”€â”€ feedforward.rs          # FFN variants
â”‚   â”‚
â”‚   â”œâ”€â”€ encoder/                    # Encoder-specific CPU impl
â”‚   â”‚   â”œâ”€â”€ mod.rs
â”‚   â”‚   â”œâ”€â”€ self_attention.rs       # EncoderSelfAttention
â”‚   â”‚   â”œâ”€â”€ encoder_layer.rs        # EncoderLayer
â”‚   â”‚   â””â”€â”€ encoder.rs              # Full encoder (CpuEncoder trait impl)
â”‚   â”‚
â”‚   â”œâ”€â”€ decoder/                    # Decoder-specific CPU impl
â”‚   â”‚   â”œâ”€â”€ mod.rs
â”‚   â”‚   â”œâ”€â”€ self_attention.rs       # DecoderSelfAttention (with KV cache)
â”‚   â”‚   â”œâ”€â”€ decoder_layer.rs        # DecoderLayer
â”‚   â”‚   â”œâ”€â”€ backend.rs              # CpuDecoderBackend
â”‚   â”‚   â””â”€â”€ generator.rs            # Token generation loop
â”‚   â”‚
â”‚   â”œâ”€â”€ encoder_decoder/            # Seq2Seq-specific CPU impl
â”‚   â”‚   â”œâ”€â”€ mod.rs
â”‚   â”‚   â”œâ”€â”€ cross_attention.rs      # CrossAttention
â”‚   â”‚   â”œâ”€â”€ cross_decoder_layer.rs  # DecoderLayer with cross-attn
â”‚   â”‚   â””â”€â”€ backend.rs              # CpuEncoderDecoderBackend
â”‚   â”‚
â”‚   â””â”€â”€ cache/                      # KV Cache implementations
â”‚       â”œâ”€â”€ mod.rs
â”‚       â”œâ”€â”€ kv_cache.rs             # Standard KVCache
â”‚       â””â”€â”€ beam_kv_cache.rs        # Double-buffered beam search cache
â”‚
â”‚
â”‚ â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
â”‚                              GPU BACKEND
â”‚ â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
â”‚
â”œâ”€â”€ gpu/
â”‚   â”‚
â”‚   â”œâ”€â”€ mod.rs                      # GPU exports
â”‚   â”‚
â”‚   â”œâ”€â”€ core/                       # GPU infrastructure
â”‚   â”‚   â”œâ”€â”€ mod.rs
â”‚   â”‚   â”œâ”€â”€ context.rs              # WgpuContext
â”‚   â”‚   â”œâ”€â”€ tensor.rs               # GpuTensor
â”‚   â”‚   â”œâ”€â”€ tensor_pool.rs          # Memory pooling
â”‚   â”‚   â””â”€â”€ frame.rs                # GpuFrameContext (per-inference)
â”‚   â”‚
â”‚   â”œâ”€â”€ shaders/                    # WGSL shader source
â”‚   â”‚   â”œâ”€â”€ matmul.wgsl
â”‚   â”‚   â”œâ”€â”€ matmul_tiled.wgsl
â”‚   â”‚   â”œâ”€â”€ softmax.wgsl
â”‚   â”‚   â”œâ”€â”€ rope.wgsl
â”‚   â”‚   â”œâ”€â”€ layer_norm.wgsl
â”‚   â”‚   â”œâ”€â”€ rms_norm.wgsl
â”‚   â”‚   â”œâ”€â”€ add.wgsl
â”‚   â”‚   â”œâ”€â”€ mul.wgsl
â”‚   â”‚   â””â”€â”€ gelu.wgsl
â”‚   â”‚
â”‚   â”œâ”€â”€ primitives/                 # Low-level GPU kernels (1:1 with shaders)
â”‚   â”‚   â”œâ”€â”€ mod.rs
â”‚   â”‚   â”œâ”€â”€ matmul.rs               # GpuMatmul kernel
â”‚   â”‚   â”œâ”€â”€ softmax.rs              # GpuSoftmax kernel
â”‚   â”‚   â”œâ”€â”€ add.rs                  # GpuAdd kernel
â”‚   â”‚   â”œâ”€â”€ mul.rs                  # GpuMul kernel
â”‚   â”‚   â”œâ”€â”€ rope.rs                 # GpuRoPE kernel
â”‚   â”‚   â””â”€â”€ broadcast.rs            # GpuBroadcast kernel
â”‚   â”‚
â”‚   â”œâ”€â”€ ops/                        # Mid-level GPU operations
â”‚   â”‚   â”œâ”€â”€ mod.rs
â”‚   â”‚   â”œâ”€â”€ linear.rs               # GpuLinearLayer
â”‚   â”‚   â”œâ”€â”€ layer_norm.rs           # GpuLayerNorm
â”‚   â”‚   â”œâ”€â”€ rms_norm.rs             # GpuRmsNorm
â”‚   â”‚   â”œâ”€â”€ rope.rs                 # GpuRoPE operation
â”‚   â”‚   â””â”€â”€ feedforward.rs          # GpuFeedForward
â”‚   â”‚
â”‚   â”œâ”€â”€ encoder/                    # Encoder-specific GPU impl
â”‚   â”‚   â”œâ”€â”€ mod.rs
â”‚   â”‚   â”œâ”€â”€ self_attention.rs
â”‚   â”‚   â”œâ”€â”€ encoder_layer.rs
â”‚   â”‚   â””â”€â”€ encoder.rs              # GpuEncoder trait impl
â”‚   â”‚
â”‚   â”œâ”€â”€ decoder/                    # Decoder-specific GPU impl
â”‚   â”‚   â”œâ”€â”€ mod.rs
â”‚   â”‚   â”œâ”€â”€ self_attention.rs
â”‚   â”‚   â”œâ”€â”€ decoder_layer.rs
â”‚   â”‚   â”œâ”€â”€ backend.rs              # GpuDecoderBackend
â”‚   â”‚   â””â”€â”€ generator.rs
â”‚   â”‚
â”‚   â”œâ”€â”€ encoder_decoder/            # Seq2Seq-specific GPU impl
â”‚   â”‚   â”œâ”€â”€ mod.rs
â”‚   â”‚   â”œâ”€â”€ cross_attention.rs
â”‚   â”‚   â”œâ”€â”€ cross_decoder_layer.rs
â”‚   â”‚   â””â”€â”€ backend.rs
â”‚   â”‚
â”‚   â””â”€â”€ cache/                      # GPU KV Cache
â”‚       â”œâ”€â”€ mod.rs
â”‚       â”œâ”€â”€ kv_cache.rs
â”‚       â””â”€â”€ beam_kv_cache.rs
â”‚
â”‚
â”‚ â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
â”‚                         ARCHITECTURE DEFINITIONS
â”‚ â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
â”‚
â”œâ”€â”€ encoder/                        # Encoder architecture (backend-agnostic)
â”‚   â”œâ”€â”€ mod.rs
â”‚   â”œâ”€â”€ traits.rs                   # CpuEncoder, GpuEncoder, EncoderLanguageModel
â”‚   â”œâ”€â”€ output.rs                   # EncoderOutput struct
â”‚   â””â”€â”€ seq2seq_encoder.rs          # Unified Seq2SeqEncoder
â”‚
â”œâ”€â”€ decoder/                        # Decoder architecture (backend-agnostic)
â”‚   â”œâ”€â”€ mod.rs
â”‚   â”œâ”€â”€ traits.rs                   # DecoderLanguageModel, CpuDecoderOps, GpuDecoderOps
â”‚   â”œâ”€â”€ backend.rs                  # AnyDecoderBackend enum
â”‚   â”œâ”€â”€ generator.rs                # DecoderGenerator (main API)
â”‚   â””â”€â”€ output.rs                   # GenerationOutput, Token
â”‚
â”œâ”€â”€ encoder_decoder/                # Seq2Seq architecture (backend-agnostic)
â”‚   â”œâ”€â”€ mod.rs
â”‚   â”œâ”€â”€ traits.rs                   # EncoderDecoderLanguageModel
â”‚   â”œâ”€â”€ backend.rs                  # AnyEncoderDecoderBackend
â”‚   â””â”€â”€ generator.rs                # Seq2SeqGenerator
â”‚
â”‚
â”‚ â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
â”‚                              MODELS
â”‚ â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
â”‚
â”œâ”€â”€ models/
â”‚   â”‚
â”‚   â”œâ”€â”€ mod.rs                      # Model exports
â”‚   â”œâ”€â”€ registry.rs                 # ModelType, ModelInfo, download
â”‚   â”œâ”€â”€ base.rs                     # ModelInput, ModelLoadConfig
â”‚   â”œâ”€â”€ factory.rs                  # Seq2SeqFactory
â”‚   â”‚
â”‚   â”œâ”€â”€ llama/                      # Llama family (Llama, Qwen, Mistral, DeepSeek)
â”‚   â”‚   â”œâ”€â”€ mod.rs
â”‚   â”‚   â”œâ”€â”€ config.rs               # LlamaConfig implements ModelConfig
â”‚   â”‚   â””â”€â”€ model.rs                # LlamaModel
â”‚   â”‚
â”‚   â”œâ”€â”€ phi/                        # Phi family
â”‚   â”‚   â”œâ”€â”€ mod.rs
â”‚   â”‚   â”œâ”€â”€ config.rs
â”‚   â”‚   â””â”€â”€ model.rs
â”‚   â”‚
â”‚   â”œâ”€â”€ gpt2/                       # GPT-2 (legacy)
â”‚   â”‚   â”œâ”€â”€ mod.rs
â”‚   â”‚   â”œâ”€â”€ config.rs
â”‚   â”‚   â””â”€â”€ model.rs
â”‚   â”‚
â”‚   â”œâ”€â”€ bert/                       # BERT family (MiniLM, MPNet, DistilBERT)
â”‚   â”‚   â”œâ”€â”€ mod.rs
â”‚   â”‚   â”œâ”€â”€ config.rs
â”‚   â”‚   â””â”€â”€ model.rs                # BertModel (for embeddings/classification)
â”‚   â”‚
â”‚   â”œâ”€â”€ bart/                       # BART family (BART, mBART, DistilBART)
â”‚   â”‚   â”œâ”€â”€ mod.rs
â”‚   â”‚   â”œâ”€â”€ config.rs
â”‚   â”‚   â””â”€â”€ model.rs
â”‚   â”‚
â”‚   â”œâ”€â”€ t5/                         # T5 family (T5, FLAN-T5)
â”‚   â”‚   â”œâ”€â”€ mod.rs
â”‚   â”‚   â”œâ”€â”€ config.rs
â”‚   â”‚   â””â”€â”€ model.rs
â”‚   â”‚
â”‚   â””â”€â”€ whisper/                    # Whisper
â”‚       â”œâ”€â”€ mod.rs
â”‚       â”œâ”€â”€ config.rs
â”‚       â”œâ”€â”€ model.rs
â”‚       â””â”€â”€ audio.rs                # AudioPipeline, MelConfig
â”‚
â”‚
â”‚ â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
â”‚                           EMBEDDINGS & COMPONENTS
â”‚ â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
â”‚
â”œâ”€â”€ embeddings/                     # Embedding layers
â”‚   â”œâ”€â”€ mod.rs
â”‚   â”œâ”€â”€ token_embeddings.rs
â”‚   â”œâ”€â”€ position_embeddings.rs
â”‚   â””â”€â”€ embedding_data.rs           # F32, BF16, quantized storage
â”‚
â”œâ”€â”€ normalization/                  # Normalization layers
â”‚   â”œâ”€â”€ mod.rs
â”‚   â”œâ”€â”€ layer_norm.rs
â”‚   â””â”€â”€ rms_norm.rs
â”‚
â”œâ”€â”€ feedforward/                    # FFN variants
â”‚   â”œâ”€â”€ mod.rs
â”‚   â”œâ”€â”€ standard.rs                 # FC1 â†’ Act â†’ FC2
â”‚   â””â”€â”€ swiglu.rs                   # Gate * Up â†’ Down
â”‚
â”œâ”€â”€ rope/                           # Rotary position embeddings
â”‚   â”œâ”€â”€ mod.rs
â”‚   â”œâ”€â”€ standard.rs
â”‚   â””â”€â”€ scaled.rs                   # LongRoPE, YaRN
â”‚
â”œâ”€â”€ linear_layer/                   # Linear layer abstraction
â”‚   â”œâ”€â”€ mod.rs
â”‚   â”œâ”€â”€ layer.rs                    # LinearLayer struct
â”‚   â”œâ”€â”€ builder.rs                  # LinearLayerBuilder
â”‚   â””â”€â”€ quantized.rs                # Q4_K, Q8_0 support
â”‚
â””â”€â”€ activations/                    # Activation functions
    â”œâ”€â”€ mod.rs
    â””â”€â”€ functions.rs                # GELU, SiLU, ReLU, etc.
```

---

## Crate Structure (Workspace)
```
kjarni/
â”œâ”€â”€ Cargo.toml                      # Workspace root
â”œâ”€â”€ README.md
â”œâ”€â”€ LICENSE-MIT
â”œâ”€â”€ LICENSE-APACHE
â”œâ”€â”€ CHANGELOG.md
â”‚
â”œâ”€â”€ crates/
â”‚   â”‚
â”‚   â”œâ”€â”€ kjarni/                     # Main library (re-exports + high-level API)
â”‚   â”‚   â”œâ”€â”€ Cargo.toml
â”‚   â”‚   â””â”€â”€ src/
â”‚   â”‚       â”œâ”€â”€ lib.rs              # Re-exports, prelude
â”‚   â”‚       â”œâ”€â”€ encoder.rs          # High-level Encoder API
â”‚   â”‚       â”œâ”€â”€ classifier.rs       # High-level Classifier API
â”‚   â”‚       â”œâ”€â”€ generator.rs        # High-level Generator API
â”‚   â”‚       â”œâ”€â”€ summarizer.rs       # High-level Summarizer API
â”‚   â”‚       â”œâ”€â”€ reranker.rs         # High-level Reranker API
â”‚   â”‚       â”œâ”€â”€ transcriber.rs      # High-level Transcriber API
â”‚   â”‚       â””â”€â”€ registry.rs         # Model management
â”‚   â”‚
â”‚   â”œâ”€â”€ kjarni-transformers/        # Core inference engine (this doc)
â”‚   â”‚   â”œâ”€â”€ Cargo.toml
â”‚   â”‚   â””â”€â”€ src/
â”‚   â”‚       â””â”€â”€ ...                 # Layout above
â”‚   â”‚
â”‚   â”œâ”€â”€ kjarni-search/              # Search functionality
â”‚   â”‚   â”œâ”€â”€ Cargo.toml
â”‚   â”‚   â””â”€â”€ src/
â”‚   â”‚       â”œâ”€â”€ lib.rs
â”‚   â”‚       â”œâ”€â”€ bm25.rs             # BM25 index
â”‚   â”‚       â”œâ”€â”€ vector.rs           # Vector similarity
â”‚   â”‚       â”œâ”€â”€ hybrid.rs           # Hybrid search (BM25 + vector)
â”‚   â”‚       â””â”€â”€ index.rs            # Persistent index
â”‚   â”‚
â”‚   â”œâ”€â”€ kjarni-cli/                 # CLI binary
â”‚   â”‚   â”œâ”€â”€ Cargo.toml
â”‚   â”‚   â””â”€â”€ src/
â”‚   â”‚       â”œâ”€â”€ main.rs
â”‚   â”‚       â”œâ”€â”€ commands/
â”‚   â”‚       â”‚   â”œâ”€â”€ mod.rs
â”‚   â”‚       â”‚   â”œâ”€â”€ model.rs
â”‚   â”‚       â”‚   â”œâ”€â”€ encode.rs
â”‚   â”‚       â”‚   â”œâ”€â”€ rerank.rs
â”‚   â”‚       â”‚   â”œâ”€â”€ search.rs
â”‚   â”‚       â”‚   â”œâ”€â”€ classify.rs
â”‚   â”‚       â”‚   â”œâ”€â”€ summarize.rs
â”‚   â”‚       â”‚   â”œâ”€â”€ translate.rs
â”‚   â”‚       â”‚   â”œâ”€â”€ transcribe.rs
â”‚   â”‚       â”‚   â”œâ”€â”€ chat.rs
â”‚   â”‚       â”‚   â””â”€â”€ generate.rs
â”‚   â”‚       â””â”€â”€ util.rs
â”‚   â”‚
â”‚   â””â”€â”€ kjarni-ffi/                 # C FFI bindings
â”‚       â”œâ”€â”€ Cargo.toml
â”‚       â”œâ”€â”€ cbindgen.toml
â”‚       â””â”€â”€ src/
â”‚           â”œâ”€â”€ lib.rs
â”‚           â”œâ”€â”€ encoder.rs
â”‚           â”œâ”€â”€ classifier.rs
â”‚           â”œâ”€â”€ generator.rs
â”‚           â”œâ”€â”€ search.rs
â”‚           â”œâ”€â”€ error.rs
â”‚           â””â”€â”€ types.rs
â”‚
â”œâ”€â”€ bindings/                       # Language-specific bindings
â”‚   â”œâ”€â”€ python/
â”‚   â”‚   â”œâ”€â”€ pyproject.toml
â”‚   â”‚   â””â”€â”€ src/
â”‚   â”œâ”€â”€ node/
â”‚   â”‚   â”œâ”€â”€ package.json
â”‚   â”‚   â””â”€â”€ src/
â”‚   â”œâ”€â”€ swift/
â”‚   â”‚   â”œâ”€â”€ Package.swift
â”‚   â”‚   â””â”€â”€ Sources/
â”‚   â””â”€â”€ csharp/
â”‚       â”œâ”€â”€ Kjarni.csproj
â”‚       â””â”€â”€ src/
â”‚
â”œâ”€â”€ examples/
â”‚   â”œâ”€â”€ simple_embedding.rs
â”‚   â”œâ”€â”€ hybrid_search.rs
â”‚   â”œâ”€â”€ rag_pipeline.rs
â”‚   â”œâ”€â”€ chat_bot.rs
â”‚   â””â”€â”€ transcribe_audio.rs
â”‚
â””â”€â”€ benches/
    â”œâ”€â”€ embedding_throughput.rs
    â”œâ”€â”€ generation_speed.rs
    â””â”€â”€ search_latency.rs

CI/CD Checklist
yaml# .github/workflows/ci.yml essentials

jobs:
  test:
    strategy:
      matrix:
        os: [ubuntu-latest, macos-latest, windows-latest]
    steps:
      - cargo fmt --check
      - cargo clippy -- -D warnings
      - cargo test --all-features
      - cargo doc --no-deps

  build-release:
    # Pre-built binaries for releases
    strategy:
      matrix:
        include:
          - os: ubuntu-latest
            target: x86_64-unknown-linux-gnu
          - os: ubuntu-latest
            target: aarch64-unknown-linux-gnu
          - os: macos-latest
            target: x86_64-apple-darwin
          - os: macos-latest
            target: aarch64-apple-darwin
          - os: windows-latest
            target: x86_64-pc-windows-msvc
```

---

## Quick Reference: What's Blocking Launch?
```
P0 (Must Have):
â”œâ”€â”€ [ ] All CLI commands working
â”œâ”€â”€ [ ] Stdin/stdout for all commands
â”œâ”€â”€ [ ] Model download/remove working
â”œâ”€â”€ [ ] README with examples
â”œâ”€â”€ [ ] cargo publish ready
â”œâ”€â”€ [ ] MIT/Apache-2.0 license
â”œâ”€â”€ [ ] CI passing on Linux/macOS/Windows
â””â”€â”€ [ ] No panics in happy path

P1 (Should Have):
â”œâ”€â”€ [ ] Pre-built binaries
â”œâ”€â”€ [ ] JSON/JSONL output formats
â”œâ”€â”€ [ ] Progress bars for downloads
â”œâ”€â”€ [ ] Helpful error messages
â”œâ”€â”€ [ ] Basic benchmarks documented
â””â”€â”€ [ ] Examples directory

P2 (Nice to Have):
â”œâ”€â”€ [ ] Homebrew formula
â”œâ”€â”€ [ ] Video demo
â”œâ”€â”€ [ ] Integration tutorials
â””â”€â”€ [ ] FFI bindings (can be v1.1)
You're close. Ship when P0 is done.