# TODO: Reusable Components for Decoder Models

## P0 - Production Hardening ‚úÖ NEW

### 1. Cancellation Tokens ‚úÖ
- [x] Create `kjarni-transformers/src/common/cancellation.rs`
- [x] `CancellationToken` + `CancellationHandle` pair
- [x] `generate_cancellable()` methods in `DecoderGenerator`
- [x] Cancellation checks in generation loop

### 2. GPU Timeout Handling ‚úÖ
- [x] Create `kjarni-transformers/src/gpu_ops/timeout.rs`
- [x] `GpuTimeoutConfig` with configurable timeout/poll interval
- [x] `poll_with_timeout_async()` for buffer mapping
- [x] Timeout-aware `read_logits_from_staging()`

### 3. Generation Statistics ‚úÖ
- [x] Create `kjarni-transformers/src/stats.rs`
- [x] `GenerationStats` with prefill/decode TPS tracking
- [x] Global enable/disable via `GenerationStats::enable()`
- [x] Integrated into `DecoderGenerator`

## P1 - High Impact

### 1. LoadedRoPE ‚úÖ (Verify)
- [x] Create `kjarni-transformers/src/rope/loader.rs`
- [x] `LoadedRoPE` struct with `cpu_rope: Arc<RoPE>`, `gpu_rope: Option<GpuRoPE>`
- [x] `new(ctx, head_dim, max_seq_len, theta, scaling, load_gpu) -> Result<Self>`
- [x] Used by: Llama, Mistral, Phi, Gemma, Qwen

### 2. DecoderPipeline Builder ‚úÖ (Verify)
- [x] Add `DecoderPipelineBuilder` to `kjarni-transformers/src/pipeline/decoder.rs`
- [x] `DecoderPipeline::builder().embeddings(...).lm_head(...).plan_from_device(...).build()`
- [x] Auto-create execution plan from device + ModelLoadConfig
- [x] Reduce boilerplate in model `from_pretrained`

### 3. DecoderModelFactory Trait ‚úÖ (Verify)
- [x] Create `kjarni-transformers/src/pipeline/loader.rs`
- [x] Trait with `load_config`, `build_backends`
- [x] `GenericLoader::load_from_pretrained<M: DecoderModelFactory>()`
- [x] Each model just implements model-specific parts

### 4. CpuLayerFactory ‚úÖ (Verify)
- [x] Create `kjarni-transformers/src/pipeline/factory.rs`
- [x] `build_norm()`, `build_swiglu_ffn()`, `build_decoder_attention()`
- [x] Layout-driven weight loading
- [x] Used by: Llama layers

## P2 - Medium Impact

### 4. TokenizerLoader
- [ ] Create `kjarni-transformers/src/tokenizer/loader.rs`
- [ ] `load_tokenizer(path, max_seq_len, padding_config) -> Result<Tokenizer>`
- [ ] Handle truncation, padding, special tokens uniformly
- [ ] Used by: All models

### 5. AttentionMaskFactory
- [ ] Create `kjarni-transformers/src/attention/mask.rs`
- [ ] `MaskType` enum: `Full`, `SlidingWindow(usize)`, `Alibi`
- [ ] `create_cpu(seq_len, past_len) -> Array2<f32>`
- [ ] `create_gpu(ctx, seq_len, max_len) -> Result<GpuTensor>`
- [ ] Used by: All models (Mistral needs SlidingWindow)

### 6. Seq2SeqPipeline
- [ ] Create `kjarni-transformers/src/pipeline/seq2seq.rs`
- [ ] `Seq2SeqPipeline` with encoder + decoder backends
- [ ] `Seq2SeqPipelineBuilder` (similar pattern to decoder)
- [ ] `Seq2SeqModelFactory` trait
- [ ] Refactor BART to use it

## P3 - Nice to Have

### 7. Fix LoadedEmbeddings API
- [ ] Make context `Option<&Arc<WgpuContext>>`
- [ ] Add `load_cpu: bool, load_gpu: bool` params
- [ ] Add `has_cpu()`, `has_gpu()` methods
- [ ] Match LoadedLMHead API pattern

### 8. Batch Generation
- [ ] Add `generate_batch()` to `DecoderGenerator`
- [ ] Batch-aware KV cache
- [ ] Batch prefill/decode in backends
- [ ] ~2-3 days effort

### 9. Backpressure Handling
- [ ] Bounded channel between generation and stream yield
- [ ] `StreamConfig` with buffer size
- [ ] ~4-8 hours effort

## Documentation

### 10. Backend Documentation ‚úÖ
- [x] `GpuDecoderBackend` - Full module docs + method docs
- [x] `CpuDecoderBackend` - Full module docs + method docs
- [x] `AnyDecoderBackend` - Type erasure explanation
- [x] `DecoderGenerator` - Usage examples + architecture diagram

### 11. README & Examples
- [ ] Main README with supported models, formats, performance
- [ ] Example: Basic chat generation
- [ ] Example: Streaming generation
- [ ] Example: Cancellation handling
- [ ] Example: Custom model loading




üèÜ The Optimization Priority Matrix
Priority	Component	Optimization	Impact	Complexity	Why it fixes "Large Models"
1	Attention	Zero-Copy KV Update	üõë Critical	Medium	Currently, forward_2 copies the entire history every token. On 7B models, this copy takes longer than the math.
2	Kernels	AVX2 RoPE (In-Place)	üî• High	Low	Allocating k_rope (size Batch*Seq*Dim) every step creates allocation jitter.
3	Kernels	AVX2 SwiGLU Fusion	üî• High	Medium	Fusing Gate * Sigmoid(Gate) * Up into one loop keeps data in L1 cache.
4	Kernels	AVX2 RMSNorm	‚ö° Medium	Low	Scalar RMSNorm is bandwidth-bound. AVX2 makes it instant.
5	Logic	Pre-Scaling Q	‚ö° Medium	Very Low	Changes 
O
(
N
)
O(N)
 math to 
O
(
1
)
O(1)
 math in the attention loop.